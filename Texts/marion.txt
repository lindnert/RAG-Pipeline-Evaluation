EFFICIENT ARGUMENT STRUCTURE EXTRACTION WITH 
TRANSFER LEARNING AND ACTIVE LEARNING 

Marion 
Thaler 


Centre for Information and Speech Processing, LMU 

marion.thaler@campus.lmu.de 


1 
Introduction 


Argument mining (AM) describes the identification of argumentative structures in documents, 
which encompasses the detection of arguments, identifying their components (e.g., claims and 
premises), and extracting relations between these components (Stab and Gurevych, 2014). Peldszus 
and Stede (2013) characterize two polar relations between argument components: support 
and attack. ”Both relations can hold between a premise and another premise” or ”a premise and 
a [...] claim” (Stab and Gurevych, 2014, p. 1504). The subsequent paper depicts relationships 
as links extending from a tail (e.g., premise) to a head (e.g., claim). Such relations are crucial 
for assessing the quality of claims (Niculae et al., 2017), which finds application in tasks such as 
policy making, summarization, or writing skill acquisition (Stab and Gurevych, 2014). Previous 
works1 have ”considered the identification of argument structures as a binary classification task 
of ordered argument component pairs” (Stab and Gurevych, 2017, p. 622), leading to a loss of 
contextual information in cases with long spans of text between argumentative pairs. However, 
methodologies addressing such long-term dependencies necessitate customized features and laborious 
expert labelling, resulting in domain-specific models and an overall scarcity of labelled 
data available within AM (Hua and Wang, 2022). 

The article discussed in the following paper, ”Efficient Argument Structure Extraction with 
Transfer Learning and Active Learning” by Hua and Wang (2022), addresses both challenges, 
encoding long-term context within a transformer-based model and alleviating data scarcity and 
domain specificity through transfer learning (TL) and active learning (AL) techniques. The overall 
aim is thus to construct a relation prediction model that, via fine-tuning, is easily adaptable to 
different domains and examine how TL and AL efficiently tackle the scarce data problem. In 
mirroring their structure, this paper will first present the data employed and then delve into the 
construction of the argument relation prediction model, followed by presenting the TL and AL 
techniques utilized. The subsequent chapters will present the results of supervised learning, TL 
and AL, respectively, followed by a summary of results and critical examination. 

2 
Datasets 
and 
Domains 


To evaluate adaptability across diverse domains, Hua and Wang (2022) employ five corpora, each 
associated with a specific domain. The initial domain, academic peer reviews, constitutes part of 
the paper’s contribution, being compiled within the research. The resultant corpus, AMPERE++, 
comprises reviews of papers presented at the International Conference on Learning Representations 
(ICLR) 2018, annotated for support and attack relations by three proficient English speakers. 

1see Stab and Gurevych (2017) and Mayer et al. (2020) 

1 


The Essays corpus encompasses persuasive essays (Stab and Gurevych, 2017). AbstRCT comprises 
biomedical paper abstracts (Mayer et al., 2020). The ECHR (Poudyal et al., 2020) includes 
legal documents from the European Court of Human Rights, while the CDCP (Park and Cardie, 
2018), the Cornell eRulemaking Corpus, consists of online user comments related to policymaking. 
Notably, the labelling of relations differs among the corpora, with ECHR and CDCP 
exclusively featuring support relations, whereas the former also contain attacks. It is important 
to highlight that within the ECHR corpus, the support relation is deduced through the provided 
labelling of premise and conclusion links. In the domains covered by the first three corpora, supplementary 
data is sourced for the Inductive TL step. Details regarding the distribution of training, 
validation, and testing splits, along with the quantity of additionally sourced unlabeled data, are 
presented in Table 1, adapted from Hua and Wang (2022, p. 427). 

AMPERE++ 
Essays 
AbstRCT 
ECHR 
CDCP 


# Doc. 400 402 700 42 731 
# Tok. 190k 147k 236k 177k 89k 
# Prop. 10,386 12,373 5,693 6,331 4,932 
# Supp. 3,370 3,613 2,402 1,946 1,426 
# Att. 266 219 70 0 0 
# Head 2,268 1,707 1,138 741 1,037 
Density 21.8% 13.8% 20.0% 11.7% 21.0% 
Splits 300/20/80 282/40/80 350/50/300 27/7/8 501/80/150 
# Add. Doc. 42k 26k 113k 0 0 

Table 1: The statistics of the five datasets include the total number of documents (# Doc.), tokens 
(# Tok.), propositions (# Prop.), support (# Supp.) and attack (# Att.) relations, unique head 
propositions (# Head.) and relation density, as well as, split distribution (training/validation/test) 
and the amount of additional unlabelled data sourced in the corresponding domain (# Add. Doc.). 

Regarding data composition, Hua and Wang (2022) find that 98% of tail propositions exhibit 
a distance that falls within a window size of 20 propositions to the left or right of the head 
proposition, irrespective of the domain. However, distinct domain-specific patterns emerge; for 
instance, AMPERE++ and CDCP tend to feature tails following heads, whereas the reverse is 
observed in AbstRCT. Essays, AbstRCT, and CDCP also exhibit a shorter distance between the 
head and tail compared to AMPERE++ and ECHR. Regarding improving transferability between 
domains, such differences in data makeup can challenge but also prove the model’s adaptability 
and, therefore, are a fitting choice for evaluation despite the lack of corresponding labelling. 

3 
Argument 
Relation 
Prediction 
Model 


The model constructed by Hua and Wang (2022) is transformer-based, containing stacked layers 
with bidirectional multi-headed self-attentions and uses RoBERTa-base (Liu et al., 2019) for encoding. 
The task consists in the prediction of support, attack 
or no-rel 
links (si 
← 
sj) 
between the proposition sj 
(head) and si 
(tail). For this, all possible head and tail pairs are considered. 
As shown in Figure 1, the input consists of a sequence of L 
propositions located to the 
left and right of a head proposition (sj), with these propositions collectively forming the context 
window. 

2 


They are separated by [CLS] 
tokens. As encoded and concatenated by Hua and Wang (2022), 
propositions can manifest at varying levels, spanning from sub-sentence units to single sentences 
and extending to multiple sentences. However, the specific level at which these propositions 
appear is not detailed in the study and is subject to variation across the different corpora. 


Figure 1: The context-aware argument relation prediction model of (Hua and Wang, 2022, p. 425) 

After the encoding, the head representation Hj 
is concatenated with each of its tail candidates 
Hi 
to be fed to the MLP (Multilayer Perceptron) output layer, where the final prediction, a softmax 
function concerning the possible three relation classes (yr): support, attack, and no-rel, 
is made: 

P 
(yr|sj,si)= 
softmax(tanh([Hj; 
Hi] 
· 
W1) 
· 
W2) 
(1) 

”The training objective is a cross-entropy loss over the label of pairwise propositions within 
the context window” (Hua and Wang, 2022, p. 426). The main difference of this model compared 
to previous relation prediction models,2 lies in the sequence-wise compared to pairwise input of 
propositions, thus being computationally less intensive,3 whilst also being able to account for 
longer text spans between a head and a tail. 

4 
Transfer 
Learning 


TL ”lessens the demand of gathering an immense amount of labelled training data by reemploying 
the knowledge gained from a different task [or data]” (Sarhan and Spruit, 2020, p. 2). The presented 
study aims to mitigate the dearth of human annotation within AM. Hua and Wang (2022) 
apply two TL techniques: Transductive TL and Inductive TL. In Transductive TL, a trained model 
is adapted to a different domain over the same task. Thus, the model is trained on a set of labelled 
data over the task of relation prediction in a source domain, e.g., legal texts, and then transferred 
to a different domain, the target domain, e.g., peer reviews (Hosna et al., 2022). 

In Inductive TL, a model is pre-trained on one task, the source task, and then transferred to 
a different task, the target task, within the same domain (Hosna et al., 2022). In this case, the 
target task is the argument relations prediction. For the source tasks, two objectives are tested: 
masked language model prediction (MLM) and context-aware sentence perturbation (Context-
Pert). MLM 
is applied according to Devlin et al. (2019), in which 15% of the input tokens are 
randomly masked and ”the objective is to predict the original vocabulary id of the masked word 
based only on its context” (Devlin et al., 2019, p. 4171). Context-Pert 
describes the process in 
which each document is perturbed by randomly replacing 20% of sentences with sentences from 
other documents and shuffling another 20% of the sentences. The aim is to predict the type of 

2see Mayer et al. (2020), Niculae et al. (2017), and Stab and Gurevych (2014) 

3Through this, the prediction complexity is reduced from O(n 
2) 
to O(nL), with n 
being the proposition count and 

L 
the context window size. 

3 


perturbation for each sentence (Hua and Wang, 2022). Both pre-training techniques are executed 
with the additionally sourced unlabelled domain data mentioned in 2. 

5 
Active 
Learning 


AL aims at ”improving data labelling efficiency” (Jeleni´c et al., 2023, p. 2282) by selecting samples 
that maximize the model’s performance. Hua and Wang (2022) employ pool-based AL. In 
each of the T 
iterations, utilizing a specific sampling heuristic, b 
samples are selected from the set 
U 
of unlabeled data, labelled, and subsequently added to the training pool Dt 
for model training. 
Since ”performing AL experiments with human annotations on-the-fly is laborious” (Margatina 
and Aletras, 2023, p. 4402), the already labelled datasets are treated as unlabeled data U. In general, 
two sampling heuristics are commonly applied in AL. The first is based on model uncertainty, 
and the second focuses on sample diversity. 

Within model uncertainty, Hua and Wang (2022) test MAX-ENTROPY, which select samples 
according to the entropy score H(.) 
computed with the model trained in the preceding iteration: 


X 


H(yr|sj, 
si)= 
− 
P 
(yr|sj, 
si) 
log 
P 
(yr|sj, 
si) 
(2) 
r 


The second method is Bayesian Active Learning by Disagreement (BALD). This method involves 
applying dropout at test time for multiple iterations over the same sample and selecting those with 
higher disagreement: 

arg 
max 
H(yr|sj, 
si) 
− 
Eθ 
[H(yr|sj, 
si,θ)] 
(3)

si 


CORESET 
ensures sample diversity by treating the selected samples in Dt 
as cluster centres. In 
each iteration, samples whose proposition representation Hi 
has the greatest L2 
distance from the 
nearest cluster centre are selected. 

As these approaches necessitate the integration of sampled data with a particular model, potentially 
limiting the generalizability of the resultant model, Hua and Wang (2022) introduce three 
supplementary model-independent strategies, namely: NOVEL-VOCAB, DISC-MARKER, and 
NO-DISC-MARKER. NOVEL-VOCAB 
selects propositions containing unseen words. Consequently, 
if a sample shares many words with samples in the labelled pool V(w), it becomes less 
probable for that sample to be selected. Thus, for each sample, the novelty score is computed by 
summing the individual word frequencies fi,t 
within the sample, normalized by the corresponding 
individual word frequencies within the labelled pool V(wt): 

X 


fi,t

novelty-score(si)= 
(4)

(1 
+ 
V(wt))

wt∈si 


DISC-MARKER 
selects samples containing any of the 18 discourse markers from the PDTB 
manual (Prasad, Rashmi et al., 2019) since discourse markers are commonly used to indicate argumentative 
relations. NO-DISC-MARKER, on the other hand, only samples propositions which 
do not contain any of the listed discourse markers. For a baseline comparison RANDOM-PROP, 
sampling a set with uniform distribution, and RANDOM-CTX, sampling all L 
propositions surrounding 
a given head until the required sampling count is reached, are employed. 

The extension to independent AL acquisition strategies by Hua and Wang aligns particularly 
well with their primary goal of achieving transferability. The incorporation of discourse markers is 
justified, as research (Stab and Gurevych, 2017) has demonstrated their effectiveness in indicating 
argument structures. However, the appropriateness of using discourse markers can vary across 
domains. This variation could potentially lead to an over-reliance on discourse markers in the 
resulting models, as the AL step does not capture less prototypical indicators of relations. 

4 


In the context of NOVEL-VOCAB, although it ostensibly ensures sample diversity, its utility 
in monological argument settings may be limited. Monological discussions often centre around 
a single issue, leading to substantial repetition of concepts and words. Consequently, the effectiveness 
of the NOVEL-VOCAB 
strategy in such settings might be diminished, selecting propositions 
which are less topical to the overall argument. 

6 
Experiments 
and 
Results 


The paper’s evaluation uses a macro-F1 score, averaged of the relation labels, whereby results on 
the ECHR and CDCP corpora only contain support 
and no-rel 
classes. In contrast, the other 
corpora include an additional attack 
relation. The experiment is conducted five times using 
different random seeds, and the mean performance scores on test sets are presented. 

6.1 
Supervised 
Learning 
Results 
In order to evaluate the standard supervised learning model utilizing the full training set, Hua and 
Wang (2022) implement three types of comparisons: SVM, SEQPAIR, and BENCHMARK. 
SVM 
constitutes a Support Vector Machine (SVM) as constructed by Stab and Gurevych (2017), 
yet eliding the essay-specific features of the original construction. The SVM is tested with linear 
and radial basis function kernels. Further, Hua and Wang (2022) adapt SEQPAIR, which uses 
the pre-trained RoBERTa and pairwise evaluates the relation of concatenated head and tail propositions 
(Devlin et al., 2019). BENCHMARK 
includes two SVMs: one tailored for Essays (Stab 
and Gurevych, 2017) and another for CDCP (Niculae et al., 2017). 


Table 2: The F1 scores for the argument prediction model with best results marked in bold 
by 
Hua and Wang (2022, p. 429). 

The model by Hua and Wang (2022) outperforms comparisons overall (Table 2), except for 
the Essays-specific SVM and SEQPAIR 
regarding AbstRCT. However, the SVM is not generalizable 
to other domains, whereas AbstRCT proves to be a highly unbalanced dataset with a much 
higher ratio of support 
or attack 
relations. Consequently, Hua and Wang (2022) interpret 
the slightly worse performance as a sign of the model’s robustness against unbalanced data. Naturally, 
the performance is best with head propositions ready supplied. Still, the end-to-end model 
outperforms the comparisons in the mentioned areas. Moreover, it is shown that the greatest context 
window of 20 propositions improves the performance the most, thus underlining the starting 
assumption that greater context is needed to grasp all relations. 

6.2 
Transfer 
Learning 
Results 
The Transductive TL results, detailed in the upper section of Table 3, indicate that AMPERE++ 
stands out as the most effective source domain. Notably, three models (Essays, ECHR, and 

5 


CDCP) exhibit performance improvements when transferred from AMPERE++. However, when 
using other source domains, the outcomes are mixed, with occasional performance declines. This 
variability is according to Hua and Wang (2022) due to domain-specific features, such as distinct 
language and style characteristics (AbstRCT, Essays) or smaller domain sizes (CDCP, ECHR). 

Still, this explanation is not entirely satisfactory since peer reviews generally follow a specific 
style and structure. 


Table 3: The TL results of Hua and Wang (2022, p. 430). The first column indicates either 
source-domain or -task. Results outperforming the supervised learning set-up of the model are 
highlighted in green. 

The results in the middle part of Table 3 reveal that within Inductive TL, MLM 
enhances 
all domains, whereas Context-Pert 
yields mixed results, only improving AMPERE++. Hua and 
Wang provide no rationale regarding these results. In the lower half of the results, Hua and Wang 
(2022) test the effectiveness of adding an Inductive TL pre-training step followed by Transductive 
TL. Hereby, the task utilized for the pre-training step is not explicitly mentioned, but it can be 
assumed to be MLM, as it consistently improved performance. 

In pre-training, utilizing the source domain specifically benefits domains with smaller datasets, 
such as ECHR and CDCP. Pre-training with the target domain yields optimal results for AMPERE++ 
and Essays. Its applicability to ECHR and CDCP is not tested due to a lack of unlabelled 
data. Still, Hua and Wang (2022) conclude that best performances can be achieved by employing 
the target domain for Inductive TL. They posit that a comprehensive understanding of the domain 
language is paramount, surpassing the importance of the source domain model. 

Moreover, Hua and Wang (2022) investigate the effectiveness of Transductive TL in lowresource 
settings by varying the training data size (0 to 5,000) across different target domains. 
AMPERE++ consistently achieves superior or competitive results, even with less than half of the 
target training set available, indicating that Transductive TL primarily benefits domains where 
little labelled data is available. 

6.3 
Active 
Learning 
Results 
Regarding AL results, Table 4 illustrates F1 scores across ten iterations, revealing gradual performance 
improvement with increased labelled data. Model-based methods (MAX-ENTROPY, 
BALD, 
CORESET) consistently outperform the comparisons, emphasizing the effectiveness of 
standard AL techniques. Still, one model-independent strategy, DISC-MARKER, yields competitive 
results, proving effective for AMPERE++ and AbstRCT. However, this is highly reliant 

6 


on the domain since the performance drops in discourse marker abundant domains, such as Essays. 
Consequently, Hua and Wang do not succeed in finding a universally adaptable modelindependent 
AL strategy. 

Further, Hua and Wang (2022) illustrate that initializing AL with checkpoints derived from 
AMPERE++ through Transductive TL enhances AL performance. This warm-start approach 
proves particularly effective in low-resource settings and showcases notable efficacy for the MAXENTROPY 
heuristic. 


Figure 2: The AL results according to different acquisition methods in 10 iterations of Hua and 
Wang (2022, p. 431). Shaded areas stand for the RANDOM-CTX 
performance. 

7 
Main 
Findings 


In summary, Hua and Wang’s research offers a three-fold contribution: demonstrating the effectiveness 
of a context-aware model that is competitive to feature-specific models, creating the 
AMPERE++ corpus, and establishing that TL enhances performance, particularly in low-resource 
settings, contingent on the selected source domain or task. Additionally, the study highlights the 
need for further exploration of model-independent AL strategies since the ones examined in the 
study only prove effective within certain domains. 

8 
Criticism 
and 
Future 
Directions 


While Hua and Wang (2022) propose efficient techniques for tackling long-term dependencies 
and data sparsity within AM, a few points within the paper are not discussed or need further development. 
For this, findings of a follow-up study by Luo et al. (2023) will also be referenced. 
Firstly, there are certain challenges associated with the data used. The overall aim of the TL step 
is to illustrate how knowledge of different domains can be used to improve performance in other 
domains. However, three of the chosen domains can be argued to belong to the realm of academia 
(AMPERE++, Essays, AbstRCT), thus possibly being very similar in language and style. Still, 
this choice in domains is very likely dictated by the availability of corpora. Still, it opens up possible 
further research areas, comparing the transferability of stylistically and contentually more 
distinct domains. 

Another problem posed by the data is the unequal distribution of relations since ”samples of 
attack are extremely scarce in the [AbstRCT] dataset” (Luo et al., 2023, p. 7567), which is probably 
also why Hua and Wang’s model consistently underperforms in this domain when compared to 
SEQPAIR. Luo et al. (2023) tackle this problem by treating the relation prediction task as binary 
within AbstRCT, namely support 
or no-rel. In the same vein, the lack of attack 
relations 
in the ECHR and the CDCP corpora impede comparability since only the macro F1 score, averaged 
over only two classes, is presented. The variation in annotation across these corpora likely 

7 


contributes to the decision not to source unlabeled data for inductive TL. Nevertheless, this aspect 
is not explicitly addressed in the paper. 

An additional point of criticism is brought up by Luo et al. (2023), who register very long 
context skewing the focus towards irrelevant sentence information. Thus, performance declines 
after a certain window size is reached; thereby, the best window size depends on the domain, as 
also mentioned by Hua and Wang (2022) when discussing the data composition (see 2). A longer 
context might introduce noise in domains with a lower distance between head and tail propositions 
(e.g., AbstRCT). This can be mitigated by randomly ”masking some sentences to enhance 
the comprehension of the contextual information”(Luo et al., 2023, p. 7565). Consequently, it 
would have been interesting to supplement the results in Table 2 with a more fine-grained context 
window sizes between 15 and 20 since here, the greatest disparities have been noted Luo et al. 
(2023). The context window’s potential limitation is also acknowledged by Hua and Wang (2022), 
as the maximum token limit of the encoder may prevent reaching the specified window size for 
certain propositions, particularly in verbose domains such as AbstRCT. 

Furthermore, the model is shown by Luo et al. to over-depend on discourse markers as an 
indicator for argumentative pairs, which might impede the model’s ability to fully leverage the 
contextual information (Luo et al., 2023, p. 7565). This can be, again, solved through data augmentation 
on the word level via randomly masking discourse markers. Further, Luo et al. (2023) 
find that performance decreases as the distance of argumentative pairs increases. Therefore, the 
model developed by Hua and Wang (2022) does not entirely solve the problem of long-term 
dependencies. To alleviate this, a ”sentence-level attention module to aggregate the contextual 
information” can be added (Luo et al., 2023, p. 7565). 

Despite these criticisms, Luo et al.’s subsequent study confirms that Hua and Wang (2022) 
succeeded in their initial goal of creating an easy-to-use framework which improves efficacy 
through context-awareness. The process and construction of the model are clearly illustrated, and 
most of the decisions made concerning TL and AL are supported via previous research, excluding 
some of the gaps mentioned above. To further investigate the observed performance enhancement 
through TL, it would be necessary to incorporate additional data from diverse domains. On the 
other hand, in AL, additional model-independent strategies could be further explored. 

References 


Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional 
transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., 
editors, Proceedings of the 2019 Conference of the North American Chapter of the Association 
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 
pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. 

Hosna, A., Merry, E., Gyalmo, J., Alom, Z., Aung, Z., and Azim, M. A. (2022). Transfer learning: 
a friendly introduction. Journal of Big Data, 9(1). 

Hua, X. and Wang, L. (2022). Efficient argument structure extraction with transfer learning 
and active learning. In Muresan, S., Nakov, P., and Villavicencio, A., editors, Findings of 
the Association for Computational Linguistics: ACL 2022, pages 423–437, Dublin, Ireland. 
Association for Computational Linguistics. 

Jelenic,´ F., Jukic,´ J., Drobac, N., and Snajder, J. (2023). On dataset transferability in active 
learning for transformers. In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of 
the Association for Computational Linguistics: ACL 2023, pages 2282–2295, Toronto, Canada. 
Association for Computational Linguistics. 

8 


Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., 
and Stoyanov, V. (2019). Roberta: A robustly optimized BERT pretraining approach. CoRR, 
abs/1907.11692. 

Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang, Y. (2023). Enhancing argument structure 
extraction with efficient leverage of contextual information. In Bouamor, H., Pino, J., and Bali, 
K., editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 
7563–7571, Singapore. Association for Computational Linguistics. 

Margatina, K. and Aletras, N. (2023). On the limitations of simulating active learning. In Rogers, 
A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for Computational 
Linguistics: ACL 2023, pages 4402–4419, Toronto, Canada. Association for Computational 
Linguistics. 

Mayer, T., Cabrio, E., and Villata, S. (2020). Transformer-based Argument Mining for Healthcare 
Applications. In ECAI 2020 -24th European Conference on Artificial Intelligence, Santiago 
de Compostela / Online, Spain. 

Niculae, V., Park, J., and Cardie, C. (2017). Argument mining with structured SVMs and RNNs. 
In Barzilay, R. and Kan, M.-Y., editors, Proceedings of the 55th Annual Meeting of the Association 
for Computational Linguistics (Volume 1: Long Papers), pages 985–995, Vancouver, 
Canada. Association for Computational Linguistics. 

Park, J. and Cardie, C. (2018). A corpus of eRulemaking user comments for measuring evaluability 
of arguments. In Calzolari, N., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Hasida, 
K., Isahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S., 
and Tokunaga, T., editors, Proceedings of the Eleventh International Conference on Language 
Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association 
(ELRA). 

Peldszus, A. and Stede, M. (2013). Ranking the annotators: An agreement study on argumentation 
structure. In Pareja-Lora, A., Liakata, M., and Dipper, S., editors, Proceedings of the 7th 
Linguistic Annotation Workshop and Interoperability with Discourse, pages 196–204, Sofia, 
Bulgaria. Association for Computational Linguistics. 

Poudyal, P., Savelka, J., Ieven, A., Moens, M. F., Goncalves, T., and Quaresma, P. (2020). ECHR: 
Legal corpus for argument mining. In Cabrio, E. and Villata, S., editors, Proceedings of the 7th 
Workshop on Argument Mining, pages 67–75, Online. Association for Computational Linguistics. 


Prasad, Rashmi, Webber, Bonnie, Lee, Alan, and Joshi, Aravind (2019). Penn discourse treebank 
version 3.0. 

Sarhan, I. and Spruit, M. (2020). Can we survive without labelled data in nlp? transfer learning 
for open information extraction. Applied Sciences, 10(17):5758. 

Stab, C. and Gurevych, I. (2014). Annotating argument components and relations in persuasive 
essays. In Tsujii, J. and Hajic, J., editors, Proceedings of COLING 2014, the 25th International 
Conference on Computational Linguistics: Technical Papers, pages 1501–1510, Dublin, 
Ireland. Dublin City University and Association for Computational Linguistics. 

Stab, C. and Gurevych, I. (2017). Parsing argumentation structures in persuasive essays. Computational 
Linguistics, 43(3):619–659. 

9 


