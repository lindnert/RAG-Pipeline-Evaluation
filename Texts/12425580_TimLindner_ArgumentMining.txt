END-TO-END ARGUMENT MINING WITH 
CROSS-CORPORA MULTI-TASK LEARNING: A REVIEW 

Tim 
Lindner 


Centre for Information and Speech Processing, LMU 

tim.lindner@campus.lmu.de 


1 
Introduction 


The goal of argumentation mining can be generalised as finding arguments in an unstructured 
text and identifying their structure (Mochales and Moens, 2011). Stab and Gurevych (2017) described 
the main tasks in argument mining (AM) as follows: i) the identification of argumentative 
spans, ii) the classification of identified argument components and thereafter iii) the identification 
and classification of the relation between such components. In other words, identifying argument 
spans means differentiating between argumentative and non-argumentative text. In order to 
achieve this, each token in the text is assigned a Beginning, Inside or Outside (B-I-O) tag. Afterwards, 
each component is classified into categories according to their role within the argument, 
such as Premise or Claim. Additionally, links between argument components can be analysed. If 
a link exists, we can classify the relation with a label, for instance Support or Attack. The above 
described tasks have to be done in sequence and are heavily dependent on each other. Some of the 
methods used in AM are derived from dependency parsing or relation-extraction methods (Morio 
et al., 2022). In the following, the paper ”End-to-end Argument Mining with Cross-corpora 
Multi-task Learning” by Morio et al. (2022) will be summarised and reviewed. 

According to Morio et al. (2022), argument mining has been gaining a lot of attention in the 
recent years. Multiple studies have investigated this field. Schulz et al. (2018), for example, 
have proven that Multi-Task Learning (MTL) can be effective for component identification in a 
low-resource setting. Similarly, Putra et al. (2021) have demonstrated a successful multi-task and 
multi-corpora strategy -however only on component link prediction without labeling relations. In 
contrast, Morio et al. (2022) were the first to cover all AM tasks jointly in their model. Moreover, 
Putra et al. (2021) only analysed rather straightforward discourses from student essays, whereas 
Morio et al. (2022) also used more complex text data, such as abstracts from medical trials. 

The main challenge in AM is a shortage of training data, particularly for new domains. According 
to Schulz et al. (2018, p.35) ”Argumentation is highly subjective and conceptualized in 
different ways”. As a result, no consistent framework exists for annotating arguments and existing 
corpora diverge in their respective annotation and specific tasks (Morio et al., 2022). In addition, 
Schulz et al. (2018) claim that it is difficult and expensive to annotate new training data reliably 
and consistently. The reason is that identifying argument structures is a complex semantic task. 
Consequently, even human annotators struggle and often disagree with each other. This has lead 
to the fact that datasets in AM are quite different to one another (Daxenberger et al., 2017). 

2 
Corpora 


In this chapter, the five different corpora used in this study will be introduced and compared. 
These corpora differ in domain, component scheme, size, relation scheme, relation number and 
complexity, as can be seen in Table 1. For example, AbstRCT is comprised of abstracts from 

1 


medical trials, whereas AAEC contains student essays about various topics. The label names as 
well as their argumentative functions vary, for example Value in CDCP versus Claim in AbstRCT. 
However, it is also visible that most of the relation labels share concepts of Support and Attack, as 
marked in Table 1 by blue and red colours. Morio et al. (2022, p. 640) refer to these transferable 
properties as ”transferability” between corpora. 


Table 1: Corpora Overview (Morio et al., 2022, p. 642) 

MTC and AASD are relatively low-resource corpora with a lower number of texts as well as 
labeled components and relations, as can be seen in Table 2). There are also certain labels that 
occur less frequently, such as Observation in AASD. 

Argument component spans are supposed to be as short as possible whilst retaining their 
sense. In order to facilitate this, AAEC and AbstRCT separate argumentative and non-argumentative 
text. Non-argumentative text are typically discourse markers, such as ”Furthermore”. In contrast 
to this, the other corpora are split into what Morio et al. (2022, p. 640) call ”segment-based 
spans”, which can be sentences or discourse clauses. 


Table 2: Corpora Statistics (Morio et al., 2022, p. 643) 

Table 2 shows that the component relations of AAEC, MTC and AASD are mapped as trees with a 
root component. In contrast, CDCP and AbstRCT are graph-based and only show transitive (also 
called re-entrant) relations (Oepen et al., 2019). Compared to its size, CDCP has more isolated 
components leading to a lower relation density. There are fewer strictly hierarchically ordered 
arguments with non-crossing relations (Oepen et al., 2019) in AbstRCT as compared to the other 
corpora, which means it has a more complex structure. 

Some of the corpora have been used before in similar studies. The AASD corpus is an enriched 
version of SciDTB, which was used by Galassi et al. (2021) along with CDCP and AbstRCT. 
Morio et al. (2022) do not state the reason for their choice of corpora. It is also interesting 
to note that they pre-processed the raw CDCP data and only used a part of the AbstRCT corpus. 

2 


3 
Single-Task 
Base 
Model 


The Single-Task (ST) Base Model is the underlying framework, which is later used for each 
corpus as part of the end-to-end Multi-Task Argument Mining (MT-AT) Pipeline described in 
Section 4. Morio et al. (2022) use an architecture that is general enough to handle the different 
structures of the five corpora to be analysed. For this reason, instead of a bi-linear model with 
two linear layers, they used a bi-affine model with one bi-linear layer, as suggested by Dozat and 
Manning (2017) and similarly to Putra et al. (2021). 


Figure 1: ST Model (Morio et al., 2022, p. 644) 

As shown at the bottom of Figure 1, there is a Longformer-base. The reason for choosing Longformer 
over BERT is that it can read longer input sequences. This may be helpful for capturing 
the relations between components in arguments that span over several sentences. On top of the 
Longformer-base, a multi-layer perceptron (MLP) classifies the BIO span tag for each token. 
Spans are represented by average pooling and later used for both component type classification as 
well as relation identification and classification. The component label is determined by another 
MLP. A bi-affine classifier predicts a relation between components and another bi-affine classifier 
the relation label. Eventually, a score matrix is produced by the link detection, which is then 
transformed into a tree or graph for visual representation (Morio et al., 2022). 

4 
Multi-Task 
Pipeline 


The Multi-Task model consists of two separate steps: i) Multi-Task pre-training and ii) Targetcorpus 
fine-tuning. This is illustrated in Figure 2. 


Figure 2: MT Pipeline (Morio et al., 2022, p. 645) 

The example shows how AbstRCT, AASD, CDCP and AAEC serve as supporting (or auxiliary) 
corpora in the pre-training step, in order to improve the model performance on the target-corpus, 

3 


which is AAEC in this example. In order to achieve this, the training data of all five corpora is 
inserted. All corpora share the common Longformer-base of the ST model described in Section 

3. However, there are specific output layers for each corpus, as each has its unique annotation 
tasks. In the next step, the pre-trained model is fine-tuned using only the target corpus training 
data. Finally, the model as a whole is evaluated only on the target corpus test data (Morio et al., 
2022). Multi-Task Learning (MTL) in this context means that a model is trained to minimise the 
loss functions of multiple corpora tasks in the pre-training step simultaneously. The idea is to 
improve performance through an information transfer from auxiliary to target corpus. 
The hyperparameters were optimised by Morio et al. (2022) and differ between MT pretraining 
and fine-tuning. For instance, they used a lower learn rate and smaller loss weight in the 
multi-task pre-training than the target corpus fine-tuning, in order to prevent a distraction through 
irrelevant information from the auxiliary corpora. 

In contrast to Figure 2, where all other four were used as auxiliary corpora (called MT-All), 
Morio et al. (2022) also tested each possible combination using only one corpus as an auxiliary, 
in order to investigate the individual effects in more detail (see Section 5). 

5 
Experiment 
Results 


First of all, Morio et al. (2022) compared their ST against other state-of-the-art models and were 
able to show overall that it performed better or comparably. Only Bert Trans. was slightly better in 
some areas, such as AAEC, matching the authors’ model in terms of universality across multiple 
corpora. 

Table 3 shows the evaluation results on the test data for each corpus comparing the ST model 
against MT-All. Because the scores of component and relation tasks depend heavily on the performance 
in span identification, Morio et al. (2022) also show the results with OS (gold) spans. This 
allows for performance tests on components and relations independently of the previous (possibly 
wrong) predictions on spans. 


Table 3: Performance Results. Statistically significant results (p < 
0.05) are marked with an 
asterisk. Results where MT-All outperformed ST are shown in bold. (Morio et al., 2022, p. 646) 

Overall MT-All performs better than ST for all five corpora. The model of Morio et al. (2022) 

4 


also has a better performance than those of Schulz et al. (2018) or Putra et al. (2021). On the other 
hand, the ST model outscored MTL in instances where the target corpus had unique labels, such 
as Policy and Testimony in CDCP. 

Most importantly however, MT-All was able to improve parsing more when targeting smaller 
corpora with less training data, such as MTC and AASD. This is in line with the findings of 
further experiments, where Morio et al. (2022) artificially reduced the target corpus training data 
down to 1% of the original.1 Figure 3 shows the result of these further experiments. The y-axis 
represents the error reduction rate (ERR), which is a weighted error reduction function that takes 
an increasing difficulty of already high ST scores into account. It is calculated as follows: 

ε(ST 
) 
− 
ε(MT 
)

ERR [%] = 
100 
× 


ε(ST 
) 


where the error reduction ε 
is 100 
− 
F-Score [%]. The results show that MT-All allowed even 
greater improvements in such an extreme scarcity scenario. Morio et al. (2022) argue that the 
difference in improvement between tasks stems from the requirement of component and relation 
tasks for more training data due to their semantic complexity. Hence, they benefit more from 
MTL in low-resource settings. In contrast, span identification is easier and can be improved with 
less training data. 


Figure 3: Low-resource ERR (Morio et al., 2022, p. 647) 

As previously mentioned, Morio et al. (2022) analysed each possible combination of single auxiliary 
corpus as well. Table 4 shows the error reduction (instead of ERR), which is defined as 
ε(ST 
) 
− 
ε(MT 
). Looking at the results, it becomes clear that the choice of the auxiliary corpus 
is crucial for improving performance. For example, CDCP was the best overall auxiliary corpus. 
The authors hypothesize that due to CDCP being graph-based instead of tree-based, it can capture 
more general argumentative relations that may be more transferable to the target corpus. In 
contrast, the smaller corpora MTC and AASD are worse as auxiliaries on average. Morio et al. 
(2022) believe that the divergence in performance of individual auxiliary corpora can be averaged 
out when using all of them at the same time (MT-All). This may be useful in case of a lacking 
in-depth knowledge about the auxiliary and/or target corpora. 
The corpora were also compared label-wise and Morio et al. (2022) observed that there are corpus 
preferences for specific labels as well. For instance, the performance for the Support label in AbstRCT 
was reduced by MTL. Morio et al. (2022) believe that there might be two reasons: Firstly, 
there was a sufficient amount of labels in the training data, so ST learning already performed 
really well. Secondly, AbstRCT is from the medical domain, which is vastly different to the other 
domains. 

1In the cases of MTC and AASD this meant a total of only one training example. 

5 


Table 4: Error reduction by auxiliary combination (Morio et al., 2022, p. 649) 

6 
Transferability 


Morio et al. (2022) identified three different types of transferability between auxiliary corpora 
and target corpus. Firstly, they identified that MTL helps to output less frequent classes. Under 
extreme data scarcity, it can happen that rarer classes, such as the ”B” tag in span identification, are 
not predicted by the model at all. MTL helps to prevent this. Another instance of this phenomenon 
would be the Example, Rebut and Undercut components in MTC, which did not occur in other 
corpora and were not labeled much in the training data. The authors were able to support this 
theory by showing that using MTL, the total number of predicted components and relations were 
higher in CDCP and AAEC when reducing training data down to 1%. 

Secondly, Morio et al. (2022) claim that the overlap of annotation schemes between corpora 
determines the performance improvement of MTL. Similarity between annotation schemes would 
allow for a direct transfer of information from the auxiliary corpus. In order to demonstrate this, 
they calculated the similarity of each single auxiliary corpus with AASD as target corpus. This 
was achieved by training the ST model on the auxiliary corpus and then testing it on AASD span 
and link detection test data. The resulting similarity was then mapped against the error reduction 
rate (ERR) of MTL vs. ST (see Figure 4). Despite mixed results, a slight positive correlation 
between similarity and parsing improvement from MTL can be observed. 


Figure 4: Relationship between similarity and ERR for target corpus AASD (Morio et al., 2022, 
p. 651) 

Finally, another hypothesis is that -given semantic compatibility -a transfer of features of the 
argument structure could be possible as well. The reason for this theory is that the two corpora 
AbstRCT and AASD were proven not to be similar in terms of annotation compatibility. In spite 
of this, when training data was reduced to 10%, there was an improvement in root component 
detection. Therefore, Morio et al. (2022) argue that there must be some kind of transfer of implicit 
knowledge of the argument structure. 

6 


7 
Conclusion 


The presented paper by Morio et al. (2022) contributed to the research in various new ways. They 
showed that their MTL model can improve performance for all three main tasks in argument mining 
and successfully overcome the challenge of training data sparsity. Moreover, they achieved 
this using corpora with complex argument structures, such as AbstRCT. On the other hand, it 
is important to keep in mind that the benefits of MTL were only proven for their specific setup 
consisting of a particular model and corpora. Further studies will be required, in order to be able 
to generalise these results. Additionally, the OS (gold span) results in Table 3 are not really representative 
of the model in a realistic scenario. The reason is that AM tasks are dependent on each 
other and the prediction on the test data should be done by the model for all tasks, not only for 
component and relation. 

Another potential issue is the comparison of their ST model with those of other papers. Different 
models were compared for different corpora and some used OS spans, whereas others did 
not. Additionally, not all of the other models included relation prediction and sometimes only 
F-Score or Macro-Score was used to compare. Thus, the comparison is not very uniform. On the 
other hand, a poor baseline ST performance would decrease the weighted ERR anyways. 

It is furthermore important to mention that the transferabilites described by Morio et al. (2022) 
are only hypotheses. In their experiment investigating a possible correlation between annotation 
compatibility and performance, MTC was completely left out and only AASD was used as target 
corpus. Moreover, the comparison was only done for spans and relations, but not for components. 
Even if a correlation exists, this does not necessarily mean causality between similarity 
and performance improvement from MTL. More research will be required, in order to leverage 
the knowledge about transferabilities for even better parsing performance. 

Finally, the Bert Trans. ST Model performed better for the AAEC corpus. Morio et al. (2022) 
therefore believe that combining a transition-based and a graph-based model might improve the 
baseline performance. Another possible direction of future research mentioned in the paper is the 
development of a silver corpus by using a trained model. 

References 


Daxenberger, J., Eger, S., Habernal, I., Stab, C., and Gurevych, I. (2017). What is the essence of a 

claim? cross-domain claim identification. In Proceedings of the 2017 Conference on Empirical 

Methods in Natural Language Processing. Association for Computational Linguistics. 

Dozat, T. and Manning, C. D. (2017). Deep biaffine attention for neural dependency parsing. 

In 5th International Conference on Learning Representations, ICLR 2017, Conference Track 

Proceedings, Toulon, France. 

Galassi, A., Lippi, M., and Torroni, P. (2021). Multi-task attentive residual networks for argument 
mining. CoRR, abs/2102.12227. 

Mochales, R. and Moens, M.-F. (2011). Argumentation mining. Artificial Intelligence and Law, 
19(1):1–22. 

Morio, G., Ozaki, H., Morishita, T., and Yanai, K. (2022). End-to-end argument mining with 

cross-corpora multi-task learning. Transactions of the Association for Computational Linguis


tics, 10:639–658. 

Oepen, S., Abend, O., Hajic, J., Hershcovich, D., Kuhlmann, M., O’Gorman, T., Xue, N., Chun, 

J., Straka, M., and Uresova, Z. (2019). Mrp 2019: Cross-framework meaning representation 

parsing. Association for Computational Linguistics. 

7 


Putra, J. W. G., Teufel, S., and Tokunaga, T. (2021). Multi-task and multi-corpora training strategies 
to enhance argumentative sentence linking performance. In Proceedings of the 8th Workshop 
on Argument Mining, pages 12–23, Punta Cana, Dominican Republic. Association for 
Computational Linguistics. 

Schulz, C., Eger, S., Daxenberger, J., Kahse, T., and Gurevych, I. (2018). Multi-task learning for 
argumentation mining in low-resource settings. In Proceedings of the 2018 Conference of the 
North American Chapter of the Association for Computational Linguistics: Human Language 
Technologies, Volume 2 (Short Papers), pages 35–41, New Orleans, Louisiana. Association for 
Computational Linguistics. 

Stab, C. and Gurevych, I. (2017). Parsing argumentation structures in persuasive essays. Computational 
Linguistics, 43(3):619–659. 

8 


